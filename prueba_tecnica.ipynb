{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creo el archivo de logs, y redirijo las salidas de los print de control para que en produccion queden como logs\n",
    "\n",
    "logging.basicConfig(filename='logs_pipeline.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "print = logger.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cargo todas las hojas (tablas) del archivo excel en un diccionario\n",
    "\n",
    "excel = pd.ExcelFile(\"Films_2 .xlsx\")\n",
    "tables = { tb:excel.parse(tb) for tb in excel.sheet_names if tb != \"MER\"}  # Excluyo la hoja del modelo lógico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creo el pipeline que ejecutará las tareas de limpieza en las tablas\n",
    "\n",
    "class PipelineClean:\n",
    "    integers_cols = (\"rental_duration\", \"length\", \"release_year\", \"num_voted_users\")\n",
    "    decimal_cols = (\"rental_rate\", \"replacement_cost\")\n",
    "\n",
    "\n",
    "    def clean_headers(self, df):\n",
    "        print(\"Limpiando encabezados\")\n",
    "        df.columns = df.columns.map(lambda x: x.strip())\n",
    "        return df\n",
    "\n",
    "\n",
    "    def clean_integers(self, df):\n",
    "        def clean(x):\n",
    "            try:\n",
    "                ok = int(x)\n",
    "            except ValueError:\n",
    "                digit = re.sub(\"\\D\", \"\", x)   # Substituyo todo lo que NO sea digito con nada... para que sea parseable a entero\n",
    "                if digit == \"\":\n",
    "                    ok = np.nan\n",
    "                else:\n",
    "                    ok = int(digit)\n",
    "            return ok\n",
    "\n",
    "        for col in df.columns:\n",
    "            if \"_id\" in col or col in self.integers_cols:\n",
    "                print(f\"Limpiando enteros, col: {col}\")\n",
    "                df[col] = df[col].apply(clean)    \n",
    "        return df\n",
    "\n",
    "\n",
    "    def clean_decimals(self, df):\n",
    "        for col in df.columns:\n",
    "            if col in self.decimal_cols:\n",
    "                print(f\"Limpiando decimales, col: {col}\")\n",
    "                df[col] = df[col].apply(lambda x: float(re.sub(\"\\D\", \"\", x)) )    \n",
    "        return df\n",
    "\n",
    "\n",
    "    def parse_dates(self, df):\n",
    "        for col in df.columns:\n",
    "            if \"date\" in col or col==\"last_update\":\n",
    "                print(f\"Casteando fechas, col: {col}\")\n",
    "                df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        return df\n",
    "\n",
    "\n",
    "    def clean_strings(self, df):\n",
    "        for col, t in df.dtypes.items():\n",
    "            if t == \"object\":\n",
    "                print(f\"Eliminando whitespaces en strings, col: {col}\")\n",
    "                df[col] = df[col].apply(lambda x: x.strip())\n",
    "        return df\n",
    "\n",
    "\n",
    "    def drop_duplicates(self, df, name):\n",
    "        print(f\"Eliminando posibles duplicados\")\n",
    "        for col in df.columns:\n",
    "            if col == name+\"_id\":\n",
    "                df.drop_duplicates(col, inplace=True)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def execute_all(self, df, name):\n",
    "        print(f\"Ejecutando pipeline para tabla: {name}\")\n",
    "        return (\n",
    "            df.pipe(self.clean_headers)\n",
    "            .pipe(self.clean_integers)\n",
    "            .pipe(self.clean_decimals)\n",
    "            .pipe(self.parse_dates)\n",
    "            .pipe(self.clean_strings)\n",
    "            .pipe(self.drop_duplicates, name)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ejecuto el pipeline para todas las tablas sobrescribiendo el mismo diccionario que las contiene\n",
    "\n",
    "pipeline = PipelineClean()\n",
    "\n",
    "for name, df in tables.items():\n",
    "    tables[name] = pipeline.execute_all(df, name)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/06 04:26:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/06 04:26:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "## Ahora que están limpias las tablas, convierto a Pyspark para hacer las consultas\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DML_Spark\").getOrCreate()\n",
    "\n",
    "for name, df in tables.items():\n",
    "    spark.createDataFrame(df).createOrReplaceTempView(name)  # Creo vista para cada tabla para usar sintaxis sql y responder las preguntas\n",
    "\n",
    "## El enunciado dice textual: \"Formular 5 posibles preguntas de negocio a las cuales los datos procesados\n",
    "# puedan dar respuesta. Responde las preguntas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|               title|Cantidad_inventario|\n",
      "+--------------------+-------------------+\n",
      "|        TORQUE BOUND|                  8|\n",
      "|       APACHE DIVINE|                  8|\n",
      "|      GILMORE BOILED|                  8|\n",
      "|         HARRY IDAHO|                  8|\n",
      "|       BOUND CHEAPER|                  8|\n",
      "|      INNOCENT USUAL|                  8|\n",
      "| EXPENDABLE STALLION|                  8|\n",
      "|     RUSH GOODFELLAS|                  8|\n",
      "|      GRIT CLOCKWORK|                  8|\n",
      "| RIDGEMONT SUBMARINE|                  8|\n",
      "|        DOGMA FAMILY|                  8|\n",
      "|    CUPBOARD SINNERS|                  8|\n",
      "|        NETWORK PEAK|                  8|\n",
      "|SWEETHEARTS SUSPECTS|                  8|\n",
      "|          PITY BOUND|                  8|\n",
      "|       MUSCLE BRIGHT|                  8|\n",
      "|       GARDEN ISLAND|                  8|\n",
      "|     VIRGINIAN PLUTO|                  8|\n",
      "|      GIANT TROOPERS|                  8|\n",
      "|       HUSTLER PARTY|                  8|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 1. Cuántas copias existen por pelicula para alquilar ?\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT title, COUNT(*) as Cantidad_inventario\n",
    "FROM film f\n",
    "JOIN inventory i\n",
    "ON f.film_id = i.film_id\n",
    "GROUP BY title\n",
    "ORDER BY Cantidad_inventario DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|Tienda|Total_peliculas|\n",
      "+------+---------------+\n",
      "|     1|              4|\n",
      "|     2|           4577|\n",
      "+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 2. Cuál es el numero de copias (inventario) de peliculas por tienda?\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT s.store_id AS Tienda,  COUNT(*) as Total_peliculas\n",
    "FROM film f\n",
    "JOIN inventory i\n",
    "ON f.film_id = i.film_id\n",
    "JOIN store s\n",
    "ON s.store_id = i.store_id    \n",
    "GROUP BY s.store_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|           title|\n",
      "+----------------+\n",
      "|ACADEMY DINOSAUR|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 3. Cuál o cuales son las peliculas que alquila la tienda 1?\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT title\n",
    "FROM film f\n",
    "JOIN inventory i\n",
    "ON f.film_id = i.film_id\n",
    "JOIN store s\n",
    "ON s.store_id = i.store_id   \n",
    "WHERE s.store_id == 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+\n",
      "|              title|Veces_alquilada|\n",
      "+-------------------+---------------+\n",
      "| BUCKET BROTHERHOOD|             34|\n",
      "|   ROCKETEER MOTHER|             33|\n",
      "|     JUGGLER HARDLY|             32|\n",
      "|RIDGEMONT SUBMARINE|             32|\n",
      "|     FORWARD TEMPLE|             32|\n",
      "+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 4. Cuales son las 5 peliculas que más se alquilaron durante el tiempo disponible en el dataset?\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT title, COUNT(*) AS Veces_alquilada\n",
    "FROM film f\n",
    "JOIN inventory i\n",
    "ON f.film_id = i.film_id \n",
    "JOIN rental r\n",
    "ON i.inventory_id = r.inventory_id\n",
    "GROUP BY title\n",
    "ORDER BY Veces_alquilada DESC\n",
    "LIMIT 5\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "|Mes|Ingreso_total|\n",
      "+---+-------------+\n",
      "|  6|    3387231.0|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 5. Cuánto fue el ingreso total de la empresa para el mes 06-2005?\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT EXTRACT(month from rental_date) AS Mes, SUM(rental_rate*rental_duration) AS Ingreso_total\n",
    "FROM film f\n",
    "JOIN inventory i\n",
    "ON f.film_id = i.film_id\n",
    "JOIN rental r\n",
    "ON i.inventory_id = r.inventory_id\n",
    "WHERE EXTRACT(month from rental_date) = 6\n",
    "GROUP BY Mes\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+\n",
      "|first_name|last_name|Total_rentas|\n",
      "+----------+---------+------------+\n",
      "|   ELEANOR|     HUNT|          46|\n",
      "|      KARL|     SEAL|          45|\n",
      "|    MARCIA|     DEAN|          42|\n",
      "|     CLARA|     SHAW|          42|\n",
      "|     TAMMY|  SANDERS|          41|\n",
      "+----------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 6. Quienes son los 5 clientes más asiduos?\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT first_name, last_name, COUNT(*) AS Total_rentas\n",
    "FROM rental r\n",
    "JOIN customer c\n",
    "ON r.customer_id = c.customer_id\n",
    "GROUP BY first_name, last_name\n",
    "ORDER BY total_rentas DESC\n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
